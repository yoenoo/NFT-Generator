{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8293a-d01d-48ec-b89a-bac323807c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import utils\n",
    "from models import UNet2DModel \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2d4a079",
   "metadata": {},
   "source": [
    "Before we proceed, please git clone the [apebase](https://github.com/skogard/apebase) repo to download 10,000 BAYC NFT images. We create a custom PyTorch dataset by implementing `__len__` and `__getitem__` methods.\n",
    "\n",
    "In the `__getitem__` method, we read in the image using PIL library, convert into PyTorch tensor in the range [0,1] and resize into 512x512x3 so that it's compatible with the autoencoder model we use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcaa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    self.root_dir = Path(root_dir)\n",
    "    self.files = glob(str(self.root_dir / \"*\"))\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.files)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    idx = idx if isinstance(idx, slice) else slice(idx, idx+1)\n",
    "    xs = self.files[idx]\n",
    "    out = []\n",
    "    for x in xs:\n",
    "      x = Image.open(x).convert(\"RGB\")\n",
    "      x = TF.to_tensor(x)\n",
    "      x = TF.resize(x, size=(512,512), antialias=True)\n",
    "      x = x.to(device)\n",
    "      out.append(x)\n",
    "    return torch.cat(out), \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ee3cee4",
   "metadata": {},
   "source": [
    "One key idea behind Stable Diffusion is \"latent diffusion\" which does the diffusion process in the \"latent space\" using the compressed representations from the autoencoder rather than raw images. These representations are information rich and can be small enough to handle on consumer hardware. Since the diffusion is done in the pixel space, which makes high-resolution image generation very computationally expensive. \n",
    "The autoencoder is trained to squish down an image into a smaller representation (encoder) and then reconstruct the image back from the compressed image (decoder). \n",
    "\n",
    "In this exercise, we will use `AutoencoderKL` from Hugging Face, where we start with a 512x512x3 image and compress to a latent vector 64x64x4 (the compression factor of 48!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27637a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install diffusers \n",
    "from diffusers import AutoencoderKL\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c1658-c817-43f6-8550-d55e8c1335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the model weights will be saved\n",
    "save_dir = Path(\"./weights\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create custom dataset\n",
    "bs = 16\n",
    "dsd = CustomDataset(root_dir=\"apebase/ipfs\")\n",
    "dls = DataLoader(dsd, batch_size=bs, shuffle=True, num_workers=0)\n",
    "\n",
    "# model instantiation\n",
    "# note the in_channels = out_channels = 4 as the model is trained on the latent images, not the raw images\n",
    "model = UNet2DModel(in_channels=4, out_channels=4, nfs=(32,64,128,256), num_layers=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 25\n",
    "tmax = epochs * len(dls)\n",
    "optimizer = Adam(model.parameters(), eps=1e-5)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "schedo = sched(optimizer)\n",
    "\n",
    "# training loop!\n",
    "model.train()\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "  batch_train_losses = []\n",
    "  pbar = tqdm(dls, mininterval=2)\n",
    "  for xb,_ in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xb = xb.to(device)\n",
    "    encoded_imgs = utils.img_to_latent(vae, xb)\n",
    "    (noised_input, t), target = utils.noisify(encoded_imgs)\n",
    "    out = model((noised_input, t))\n",
    "    loss = F.mse_loss(out, target)\n",
    "    loss.backward()\n",
    "    loss.detach()\n",
    "    schedo.optimizer.step()\n",
    "    schedo.step()\n",
    "    batch_train_losses.append(loss.item())\n",
    "    pbar.set_description(f\"loss {loss.item():.2f}\")        \n",
    "\n",
    "  train_losses.extend(batch_train_losses)\n",
    "  print(f\"Epoch {epoch}, loss: {np.mean(train_losses)}\")\n",
    "\n",
    "  # save the model weights every 4 epochs\n",
    "  if epoch % 4 == 0 or epoch == int(epochs-1):\n",
    "    model_path = save_dir / f\"bayc_model_{epoch}_bs_{bs}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"saved model at {model_path.absolute().as_posix()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
